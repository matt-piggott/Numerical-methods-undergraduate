{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Methods\n",
    "\n",
    "## Numerical Linear Algebra II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as sl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives:\n",
    "\n",
    "* More on *direct* linear solver methods: the LU decomposition of a matrix\n",
    "\n",
    "\n",
    "* Doolittle's algorithm\n",
    "\n",
    "\n",
    "* Properties of lower-triangular matrices\n",
    "\n",
    "\n",
    "* Partial pivoting\n",
    "\n",
    "* Introduce ill-conditioned matrices (via matrix norms and the condition number)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Last week we developed and implemented the *Gaussian elimination* method to solve the linear matrix system \n",
    "\n",
    "$A\\boldsymbol{x}=\\boldsymbol{b}$. \n",
    "\n",
    "This week we will consider a closely related solution method: *LU decomposition* or *LU factorisation*.\n",
    "\n",
    "Both are examples of *direct* solution methods - in the next lecture we will consider the alternative approach to solve linear systems, namely *iterative* or *indirect* methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## LU decomposition - motivation\n",
    "\n",
    "Last week we implemented Gaussian elimination to solve the matrix system with *one* RHS vector $\\boldsymbol{b}$.  \n",
    "\n",
    "It is often that case that we have to deal with problems where we have multiple RHS vectors, all with the same matrix $A$.\n",
    "\n",
    "We could call the same code (e.g. our 'upper_triangle' and 'back_substitution' codes from last lecture) multiple times to solve all of these corresponding linear systems, but note that as the elimination algorithm is actually performing operations based upon (the same) $A$ each time, we would be wasting time repeating exactly the same operations over and over again - this is therefore clearly not an efficient solution to this problem.\n",
    "\n",
    "We could easily generalise our Gaussian elimination/back substitution algorithms to include multiple RHS column vectors in the augmented system, perform the same sequence of row operations (but now only once) to transform the matrix to upper-triangular form, and then perform back substitution on each of the transformed RHS vectors from the augmented system - cf. the use of Gaussian elimination to compute the inverse to a matrix by placing the identity matrix (or equivalently the columns of the identity matrix) on the right of the augmented system: $[A|\\, \\textrm{col}_1,\\, \\textrm{col}_2, \\ldots]$.\n",
    "\n",
    "However, it is often the case that each RHS vector depends on the solutions to the matrix systems obtained from some or all of the earlier RHS vectors, and so this generalisation would not work in this case. For example, our discrete system could be of the form\n",
    "\n",
    "$$A\\boldsymbol{x}^{n+1} = \\boldsymbol{b}(\\boldsymbol{x}^n, \\ldots)$$\n",
    "\n",
    "where $\\boldsymbol{x}^{n+1}$ is the numerical solution of a (ordinary or partial) differential equation at time level $n+1$, the RHS is a function of the the solution at the previous time level $n$ (and possibly other things such as forcing terms, represented by the \"$\\ldots$\"). $A$ is then a discretisation of the differential equation written in the form that it maps the old solution to the new one. Time stepping this problem involves solving the same linear system multiple times, but with different RHSs. \n",
    "\n",
    "[NB. this is true if our model is *linear*, in the nonlinear case $A$ would be updated every time step]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU decomposition - theory\n",
    "\n",
    "To deal with this situation efficiently we *decompose* or *factorise* the matrix $A$ in such a way that it is cheap to compute a new solution vector $\\boldsymbol{x}$ for any given RHS vector $\\boldsymbol{b}$.  This decomposition involves a lower- and an upper-triangular matrix, hence the name LU decomposition. These matrices essentially *encode* the steps conducted in Gaussian elimination, so we don't have to explicilty conduct all of the operations again and again.\n",
    "\n",
    "Mathematically, let's assume that we have already found/constructed a lower-triangular matrix ($L$ - where all entries above the diagonal are zero) and an upper-triangular matrix ($U$ - where all entries below the diagonal are zero) such that we can write\n",
    "\n",
    "$$ A = LU $$\n",
    "\n",
    "In this case the matrix system we need to solve for $\\boldsymbol{x}$ becomes\n",
    "\n",
    "$$ A\\boldsymbol{x} = \\boldsymbol{b} \\iff (LU)\\boldsymbol{x} = \\boldsymbol{b} \\iff  L(U\\boldsymbol{x}) = \\boldsymbol{b} $$\n",
    "\n",
    "Notice that the matrix-vector product $U\\boldsymbol{x}$ is itself a vector, let's call it $\\boldsymbol{c}$ for the time-being (i.e. \n",
    "$\\boldsymbol{c}=U\\boldsymbol{x}$).\n",
    "\n",
    "The above system then reads \n",
    "\n",
    "$$ L\\boldsymbol{c} = \\boldsymbol{b} $$\n",
    "\n",
    "where $L$ is a matrix and $\\boldsymbol{c}$ is an unknown.  \n",
    "\n",
    "As $L$ is in lower-triangular form we can use forward substitution (generalising the back subsitution algorithm/code we developed last week) to very easily find $\\boldsymbol{c}$ in relatively few operations (we don't need to call the entire Gaussian elimination algorithm).\n",
    "\n",
    "Once we know $\\boldsymbol{c}$ we then solve the second linear system \n",
    "\n",
    "$$ U\\boldsymbol{x} = \\boldsymbol{c} $$\n",
    "\n",
    "where now we can use the fact that $U$ is upper-triangular to use our back substitution algorithm again very efficiently to give the solution $\\boldsymbol{x}$ we require.\n",
    "\n",
    "So for a given $\\boldsymbol{b}$ we can find the corresponding $\\boldsymbol{x}$ very efficiently, we can therefore do this repeatedly as each new $\\boldsymbol{b}$ is given to us.\n",
    "\n",
    "Our challenge is therefore to find the matrices $L$ and $U$ that form the decomposition $A=LU$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU decomposition - algorithm\n",
    "\n",
    "Recall the comment above on the $L$ and $U$ matrices encoding the steps taken in Gaussian elimination.  Let's see how this works through the development of the so-called *Doolittle algorithm*.\n",
    "\n",
    "Let's consider an example matrix:\n",
    "\n",
    "$$\n",
    "  A=\\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "{\\color{black}5} & {\\color{black}14} & {\\color{black}7} & {\\color{black}10}\\\\\n",
    "{\\color{black}20} & {\\color{black}77} & {\\color{black}41} & {\\color{black}48}\\\\\n",
    "{\\color{black}25} & {\\color{black}91} & {\\color{black}55} & {\\color{black}67}\\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "the first step of Gaussian elimination is to set the\n",
    "sub-diagonal elements in the first column to zero by subtracting multiples of\n",
    "the first row from each of the subsequent rows. \n",
    "\n",
    "For this example, using the symbolic notiation from last week\n",
    "this requires the row operations\n",
    "\n",
    "\\begin{align}\n",
    "Eq. (2) &\\leftarrow Eq. (2) - 1\\times Eq. (1)\\\\\n",
    "Eq. (3) &\\leftarrow Eq. (3) - 4\\times Eq. (1)\\\\\n",
    "Eq. (4) &\\leftarrow Eq. (4) - 5\\times Eq. (1)\\\\\n",
    "\\end{align}\n",
    "\n",
    "or mathematically, and for each element of the matrix (remembering that we are adding rows together - while one of\n",
    "the entries of a row will end up being zero, this also has the consequence of updating the rest of the values in that row, hence the iteration over $j$ below):\n",
    "\n",
    "\\begin{align}\n",
    "A_{2j} &\\leftarrow A_{2j} - \\frac{A_{21}}{A_{11}} A_{1j} = A_{2j} - \\frac{5}{5} \\times A_{1j} = A_{2j} \\, {\\color{Orange}{-1}} \\times A_{1j}, \\quad j=1,2,3,4\\\\\n",
    "A_{3j} &\\leftarrow A_{3j} - \\frac{A_{31}}{A_{11}} A_{1j} = A_{3j} - \\frac{20}{5} \\times A_{1j} = A_{3j}\\, {\\color{Orange}{-4}} \\times A_{1j} , \\quad j=1,2,3,4\\\\\n",
    "A_{4j} &\\leftarrow A_{4j} - \\frac{A_{41}}{A_{11}} A_{1j} = A_{4j} - \\frac{25}{5} \\times A_{1j}= A_{4j}\\, {\\color{Orange}{-5}} \\times A_{1j}, \\quad j=1,2,3,4\\\\\n",
    "\\end{align}\n",
    "\n",
    "<br>\n",
    "\n",
    "Notice that we can also write these exact operations on elements in terms of multiplication by a carefully chosen lower-triangular matrix where the non-zero's below the diagonal are restricted to a single column, e.g. for the example above\n",
    "\n",
    "$$\n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}1} & {\\color{black}0} & {\\color{black}0} & {\\color{black}0}\\\\\n",
    "    {\\color{Orange}{-1}} & {\\color{black}1} & {\\color{black}0} & {\\color{black}0}\\\\\n",
    "    {\\color{Orange}{-4}} & {\\color{black}0} & {\\color{black}1} & {\\color{black}0}\\\\\n",
    "    {\\color{Orange}{-5}} & {\\color{black}0} & {\\color{black}0} & {\\color{black}1}\\\\   \n",
    "  \\end{bmatrix}\\qquad\\times\\qquad\\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{black}5} & {\\color{black}14} & {\\color{black}7} & {\\color{black}10}\\\\\n",
    "    {\\color{black}20} & {\\color{black}77} & {\\color{black}41} & {\\color{black}48}\\\\\n",
    "    {\\color{black}25} & {\\color{black}91} & {\\color{black}55} & {\\color{black}67}\\\\   \n",
    "  \\end{bmatrix}\\qquad=\\qquad\\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{blue}{0}} & {\\color{blue}{7}} & {\\color{blue}{2}} & {\\color{blue}{1}}\\\\\n",
    "    {\\color{blue}{0}} & {\\color{blue}{49}} & {\\color{blue}{21}} & {\\color{blue}{12}}\\\\\n",
    "    {\\color{blue}{0}} & {\\color{blue}{56}} & {\\color{blue}{30}} & {\\color{blue}{22}}\\\\    \n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "the blue portion of the matrix just highlights those entries that have been affected - notice that the first column of this submatrix is by construction all zero.\n",
    "\n",
    "The lower-triangular matrix we pre-multiply by in the above (let's call this one $L_0$) is thus encoding the first step in Gaussian elimination.\n",
    "\n",
    "<br>\n",
    "\n",
    "The next step involves taking the second row of the updated matrix as the new pivot (we will ignore partial pivoting for simplicity), and subtracting multiples of this row from those below in order to set the zeros below the diagonal in the second column to zero. \n",
    "\n",
    "Note that this step can be achieved here with the multiplication by the following lower-triangular matrix (call this one $L_1$)\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}1} & {\\color{black}0} & {\\color{black}0} & {\\color{black}0}\\\\\n",
    "    {\\color{black}0} & {\\color{black}1} & {\\color{black}0} & {\\color{black}0}\\\\\n",
    "    {\\color{black}0} & {\\color{Orange}{-7}} & {\\color{black}1} & {\\color{black}0}\\\\\n",
    "    {\\color{black}0} & {\\color{Orange}{-8}} & {\\color{black}0} & {\\color{black}1}\\\\\n",
    "  \\end{bmatrix}\\qquad\\times\\qquad\\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{black}0} & {\\color{black}7} & {\\color{black}2} & {\\color{black}1}\\\\\n",
    "    {\\color{black}0} & {\\color{black}49} & {\\color{black}21} & {\\color{black}12}\\\\\n",
    "    {\\color{black}0} & {\\color{black}56} & {\\color{black}30} & {\\color{black}22}\\\\\n",
    "  \\end{bmatrix}\\qquad=\\qquad\\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{black}0} & {\\color{black}7} & {\\color{black}2} & {\\color{black}1}\\\\\n",
    "    {\\color{black}0} & {\\color{blue}{0}} & {\\color{blue}{7}} & {\\color{blue}{5}}\\\\\n",
    "    {\\color{black}0} & {\\color{blue}{0}} & {\\color{blue}{14}} & {\\color{blue}{14}}\\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "And finally for this example to get rid the of the leading 14 on the final row:\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}1} & {\\color{black}0} & {\\color{black}0} & {\\color{black}0}\\\\\n",
    "    {\\color{black}0} & {\\color{black}1} & {\\color{black}0} & {\\color{black}0}\\\\\n",
    "    {\\color{black}0} & {\\color{black}0} & {\\color{black}1} & {\\color{black}0}\\\\\n",
    "    {\\color{black}0} & {\\color{black}0} & {\\color{Orange}{-2}} & {\\color{black}{1}}\\\\\n",
    "  \\end{bmatrix}\\qquad\\times\\qquad\\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{black}0} & {\\color{black}7} & {\\color{black}2} & {\\color{black}1}\\\\\n",
    "    {\\color{black}0} & {\\color{black}0} & {\\color{black}7} & {\\color{black}5}\\\\\n",
    "    {\\color{black}0} & {\\color{black}0} & {\\color{black}14} & {\\color{black}14}\\\\\n",
    "  \\end{bmatrix}\\qquad=\\qquad\\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{black}0} & {\\color{black}7} & {\\color{black}2} & {\\color{black}1}\\\\\n",
    "    {\\color{black}0} & {\\color{black}0} & {\\color{black}7} & {\\color{black}5}\\\\\n",
    "    {\\color{black}0} & {\\color{black}0} & {\\color{blue}{0}} & {\\color{blue}{4}}\\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "where this lower triangular matrix we will call $L_2$, and the RHS matrix is now in upper-triangular form as we expect from Gaussian elimination (call this $U$).\n",
    "\n",
    "In summary, the above operations (starting from $A$, first multiplying on the left by $L_0$, then multiplying the result on the left by $L_1$ and so on to arrive at an upper triangular matrix $U$) can be written as \n",
    "\n",
    "$$ L_2(L_1(L_0A)) = U $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these special lower-triangular matrices $L_0$, $L_1$ etc, are all examples of what is known as *atomic* lower-triangular matrices: a special form of unitriangular matrix (a triangular matrix with the diagonals all being unity)  where the off-diagonal entries are all zero apart from in a single column. \n",
    "\n",
    "Notice that for these special, simple matrices, their inverse is simply the original matrix but with the sign of those off-diagonals changed:\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "  \\begin{array}{rrrrrrrrr}\n",
    "    1      & 0      & \\cdots      &        &              &        &         &   & 0 \\\\\n",
    "    0      & 1      & 0           & \\cdots &              &        &         &   & 0 \\\\\n",
    "    0      & \\ddots & \\ddots      & \\ddots &              &        &         &   &  \\vdots \\\\\n",
    "    \\vdots & \\ddots & \\ddots      & \\ddots &              &        &         &   &  \\\\\n",
    "           &        &             &   0    &   1          &  0     &         & &  &  \\\\           \n",
    "           &        &             &   0    &   l_{i+1,i}  &  1     &  \\ddots &   &  &  \\\\  \n",
    "           &        &             &   0    &   l_{i+2,i}  &  0     &  \\ddots &   & &  \\\\  \n",
    "           &        &             & \\vdots &   \\vdots     & \\vdots &  \\ddots &   & 0 &  \\\\               \n",
    "    0      & \\cdots &             & 0      &   l_{n,i}    & 0      &  \\cdots & 0 & 1 &  \\\\    \n",
    "\\end{array}\n",
    "\\right]^{-1}\n",
    "=\n",
    "\\left[\n",
    "  \\begin{array}{rrrrrrrrr}\n",
    "    1      & 0      & \\cdots      &        &              &        &         &   & 0 \\\\\n",
    "    0      & 1      & 0           & \\cdots &              &        &         &   & 0 \\\\\n",
    "    0      & \\ddots & \\ddots      & \\ddots &              &        &         &   &  \\vdots \\\\\n",
    "    \\vdots & \\ddots & \\ddots      & \\ddots &              &        &         &   &  \\\\\n",
    "           &        &             &   0    &   1          &  0     &         & &  &  \\\\           \n",
    "           &        &             &   0    &   -l_{i+1,i}  &  1     &  \\ddots &   &  &  \\\\  \n",
    "           &        &             &   0    &   -l_{i+2,i}  &  0     &  \\ddots &   & &  \\\\  \n",
    "           &        &             & \\vdots &   \\vdots     & \\vdots &  \\ddots &   & 0 &  \\\\               \n",
    "    0      & \\cdots &             & 0      &   -l_{n,i}    & 0      &  \\cdots & 0 & 1 &  \\\\    \n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "This means we can write down the product of their inverses trivially:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Exercise 1: lower-triangular matrices</span>\n",
    "\n",
    "Convince yourselves of the following facts:\n",
    "\n",
    "\n",
    "* The above statement on what the inverse of the special $L$ matrices is - check on a small example.\n",
    "\n",
    "\n",
    "* The multiplication of arbitrary lower-triangular square matrices is also lower-triangular.\n",
    "\n",
    "\n",
    "* $L_2(L_1(L_0A)) = U \\implies A = L_0^{-1}(L_1^{-1}(L_2^{-1}U))$\n",
    "\n",
    "\n",
    "* and hence that $A=LU$ where $U$ is the upper-triangular matrix found at the end of Gaussian elimination, and where $L$ is the following  matrix\n",
    "\n",
    "$$ L = L_0^{-1}L_1^{-1}L_2^{-1} $$\n",
    "\n",
    "\n",
    "* Finally, compute this product of these lower-triangular matrices to show that \n",
    "\n",
    "$$L = \n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}1} & {\\color{black}0} & {\\color{black}0} & {\\color{black}0}\\\\\n",
    "    {\\color{black}{1}} & {\\color{black}1} & {\\color{black}0} & {\\color{black}0}\\\\\n",
    "    {\\color{black}{4}} & {\\color{black}7} & {\\color{black}1} & {\\color{black}0}\\\\\n",
    "    {\\color{black}{5}} & {\\color{black}8} & {\\color{black}2} & {\\color{black}1}\\\\   \n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "i.e. that the multiplication of these individual atomic matrices (importantly in this order) simply merges the entries from the non-zero columns of each atomic matrix, and hence is both lower-triangular, as well as trivial to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU decomposition - implementation\n",
    "\n",
    "So we can build an LU code easily from our Gaussian elimination code from last lecture which already works out and performs these tasks.  \n",
    "\n",
    "The final $U$ matrix we need here is as was already constructed through Gaussian elimination, the entries of $L$ we need are simply the ${A_{ik}}/{A_{kk}}$ multipliers we computed as part of the elimination, but threw away previously (we called them `s` in the code and computed them outside of the `j` loop).\n",
    "\n",
    "<br>\n",
    "\n",
    "***Implementation observation***\n",
    "\n",
    "For a given pivot row $k$, for each of these multipliers (for every row below the pivot), as we compute them we know that we are going to transform the augmented matrix in order to achieve a new zero below the diagonal - this frees up space in the matrix that we could use to store each multiplier moving on to the following row, we implicitly know that the diagonals of $L$ will be unity and so don't need to store these (and noting that we don't actually have a space for them anyway!). We then move on to the following pivot row, replacing the zeros in the corresponding column we are zero'ing with the multipliers again.\n",
    "\n",
    "For example, for the case above \n",
    "\n",
    "$$ A = \n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{black}5} & {\\color{black}14} & {\\color{black}7} & {\\color{black}10}\\\\\n",
    "    {\\color{black}20} & {\\color{black}77} & {\\color{black}41} & {\\color{black}48}\\\\\n",
    "    {\\color{black}25} & {\\color{black}91} & {\\color{black}55} & {\\color{black}67}\\\\   \n",
    "  \\end{bmatrix}\\quad\\rightarrow\\quad\n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{blue}{1}} & {\\color{black}{7}} & {\\color{black}{2}} & {\\color{black}{1}}\\\\\n",
    "    {\\color{blue}{4}} & {\\color{black}{49}} & {\\color{black}{21}} & {\\color{black}{12}}\\\\\n",
    "    {\\color{blue}{5}} & {\\color{black}{56}} & {\\color{black}{30}} & {\\color{black}{22}}\\\\    \n",
    "  \\end{bmatrix}\\quad\\rightarrow\\quad\n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{blue}1} & {\\color{black}7} & {\\color{black}2} & {\\color{black}1}\\\\\n",
    "    {\\color{blue}4} & {\\color{blue}{7}} & {\\color{black}{7}} & {\\color{black}{5}}\\\\\n",
    "    {\\color{blue}5} & {\\color{blue}{8}} & {\\color{black}{14}} & {\\color{black}{14}}\\\\\n",
    "  \\end{bmatrix}\\quad\\rightarrow\\quad\n",
    "  \\begin{bmatrix}\n",
    "    {\\color{black}5} & {\\color{black}7} & {\\color{black}5} & {\\color{black}9}\\\\\n",
    "    {\\color{blue}1} & {\\color{black}7} & {\\color{black}2} & {\\color{black}1}\\\\\n",
    "    {\\color{blue}4} & {\\color{blue}7} & {\\color{black}7} & {\\color{black}5}\\\\\n",
    "    {\\color{blue}5} & {\\color{blue}8} & {\\color{blue}{2}} & {\\color{black}{4}}\\\\\n",
    "  \\end{bmatrix}\n",
    "  = [\\color{blue}L\\backslash U]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Exercise 2: LU decomposition</span>\n",
    "\n",
    "Starting from your Gaussian elimination code (or rather the `upper_triangle` part) produce a new code to compute the LU decomposition of a matrix. First, store $L$ and $U$ in two different matrices; you could then try a version where you store the entries of both $L$ and $U$ in $A$ as described above.\n",
    "\n",
    "NB. In my sample solution I actually don't use the trick of using the zero'd column to store the multipliers, I create a new $L$ array to store this, and return this $L$ along with the upper triangular $U$ (which will be the transformed $A$ array) from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial pivoting\n",
    "\n",
    "At the end of last week we commented that a problem would occur in a numerical implementation of our algorithm if we had a situation where the $A_{kk}$ we divide through by in the Gaussian elimination and/or back substitution algorithms was zero (or close to zero).\n",
    "\n",
    "Using Gaussian elimination as an example, let's again consider the algorithm mid-way working on an arbitrary matrix system, i.e. assume that the first $k$ rows have already been transformed into upper-triangular form, while the equations/rows below are not yet in this form:\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "  \\begin{array}{rrrrrrr|r}\n",
    "    A_{11} & A_{12} & A_{13} & \\cdots & A_{1k}  & \\cdots & A_{1n} & b_1 \\\\\n",
    "    0      & A_{22} & A_{23} & \\cdots & A_{2k} & \\cdots & A_{2n} & b_2 \\\\\n",
    "    0      & 0      & A_{33} & \\cdots & A_{3k}  & \\cdots & A_{3n} & b_3 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  & \\ddots & \\vdots & \\vdots \\\\\n",
    "\\hdashline    \n",
    "    0      & 0      & 0      & \\cdots & A_{kk}  & \\cdots & A_{kn} & b_k \\\\    \n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots  & \\ddots & \\vdots & \\vdots \\\\\n",
    "    0      & 0      & 0      & \\cdots & A_{nk}  & \\cdots & A_{nn} & b_n \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Note we have drawn the horizontal dashed line one row higher than we did previously, as we are not going to simply assume without first checking that it is wise to take the current row $k$ as the pivot row, and $A_{kk}$ as the so-called pivot element.\n",
    "\n",
    "We have a problem if $A_{kk}$ is zero as we can't use it to cancel entries below.\n",
    "\n",
    "*Partial pivoting* is a *trick* that can be used to fix this problem. It works by using the fact that we can swap rows - we select the best pivot (row or element) as the one where the $A_{ik}$ (for $i\\ge k$) value is largest (relative to the other values of components in its own row $i$), and then swaps this row with the current $k$ row.\n",
    "\n",
    "To generalise the codes we have written so far we would simply need to search for this row, and perform the row swap operation.\n",
    "\n",
    "Python's `scipy.linalg` library has its own implementation of the LU decomposition, and it uses partial pivoting by default - this is why it wasn't straightforward to compare our existing codes against the output of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.  7.  5.  9.]\n",
      " [ 5. 14.  7. 10.]\n",
      " [20. 77. 41. 48.]\n",
      " [25. 91. 55. 67.]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[ 5., 7.,   5.,  9.],\n",
    "               [ 5., 14.,  7., 10.],\n",
    "               [20., 77., 41., 48.],\n",
    "               [25., 91. ,55., 67.]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "P, L , U = sl.lu(A)\n",
    "\n",
    "# P here is a 'permutation matrix' that performs swaps based upon partial pivoting\n",
    "print(P)  \n",
    "\n",
    "# the fact that it isn't just the identity means that the scipy algorithm did indeed decide \n",
    "# to perform row swaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.          0.          0.        ]\n",
      " [ 0.2         1.          0.          0.        ]\n",
      " [ 0.8        -0.375       1.          0.        ]\n",
      " [ 0.2         0.375       0.33333333  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# the lower-triangular matrix\n",
    "print(L)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 25.          91.          55.          67.        ]\n",
      " [  0.         -11.2         -6.          -4.4       ]\n",
      " [  0.           0.          -5.25        -7.25      ]\n",
      " [  0.           0.           0.           0.66666667]]\n"
     ]
    }
   ],
   "source": [
    "# the upper-triangular matrix\n",
    "print(U)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.  7.  5.  9.]\n",
      " [ 5. 14.  7. 10.]\n",
      " [20. 77. 41. 48.]\n",
      " [25. 91. 55. 67.]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# double check that P*L*U does indeed equal A\n",
    "print(P @ L @ U)\n",
    "print(np.allclose(A, P@L@U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the form of $P$ above, we can actually re-order the rows of $A$ in advance and consider the LU decomposition of the matrix where $P=I$, as below. \n",
    "\n",
    "What I mean here is that since $A = PLU$, and as $P^{-1}=P^T$, that $P^T A = LU$, so we should reorder $A$ based on what pre-multiplication by the transpose of $P$ does.\n",
    "\n",
    "As we haven't bothered implementing pivoting ourselves yet in our own codes, we can check that our LU implementations (without partial pivoting) recreates the $A$, $L$ and $U$ below after we've done this pre-re-ordering of $A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25. 91. 55. 67.]\n",
      " [ 5.  7.  5.  9.]\n",
      " [20. 77. 41. 48.]\n",
      " [ 5. 14.  7. 10.]]\n"
     ]
    }
   ],
   "source": [
    "# original A\n",
    "#A = np.array([[ 5., 7.,   5.,  9.],\n",
    "#               [ 5., 14.,  7., 10.],\n",
    "#               [20., 77., 41., 48.],\n",
    "#               [25., 91. ,55., 67.]])\n",
    "\n",
    "# a new A with swapped rows based on observation of P\n",
    "A = np.array([[25. ,91. ,55. ,67.],\n",
    "               [ 5.,  7.,  5.,  9.], \n",
    "               [20., 77., 41., 48.],\n",
    "               [ 5., 14., 7.,  10.]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25. 91. 55. 67.]\n",
      " [ 5.  7.  5.  9.]\n",
      " [20. 77. 41. 48.]\n",
      " [ 5. 14.  7. 10.]]\n"
     ]
    }
   ],
   "source": [
    "# this is where this comes from\n",
    "A = np.array([[ 5., 7.,   5.,  9.],\n",
    "               [ 5., 14.,  7., 10.],\n",
    "               [20., 77., 41., 48.],\n",
    "               [25., 91. ,55., 67.]])\n",
    "print(P.T@A)\n",
    "\n",
    "# we use the transpose of P since A = PLU -> P^T A = LU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# a new A with swapped rows based on observation of P\n",
    "A = np.array([[25. ,91. ,55. ,67.],\n",
    "               [ 5.,  7.,  5.,  9.], \n",
    "               [20., 77., 41., 48.],\n",
    "               [ 5., 14., 7.,  10.]])\n",
    "\n",
    "\n",
    "P, L, U = sl.lu(A)\n",
    "# P now should be the identity matrix as pivoting no longer actually actions any row swaps with this A\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.          0.          0.        ]\n",
      " [ 0.2         1.          0.          0.        ]\n",
      " [ 0.8        -0.375       1.          0.        ]\n",
      " [ 0.2         0.375       0.33333333  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# so this is the L we can compare our non-PP based implemetation against if we test it on this A\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 25.          91.          55.          67.        ]\n",
      " [  0.         -11.2         -6.          -4.4       ]\n",
      " [  0.           0.          -5.25        -7.25      ]\n",
      " [  0.           0.           0.           0.66666667]]\n"
     ]
    }
   ],
   "source": [
    "# and the corresponding U\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25. 91. 55. 67.]\n",
      " [ 5.  7.  5.  9.]\n",
      " [20. 77. 41. 48.]\n",
      " [ 5. 14.  7. 10.]]\n"
     ]
    }
   ],
   "source": [
    "print(P @ L @ U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Optional Exercise 3: Partial pivoting</span>\n",
    "\n",
    "Implement partial pivoting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ill-conditioned matrices\n",
    "\n",
    "The conditioning (or lack of, i.e. the ill-conditioning) of matrices we are trying to invert - to obtain the inverse, or to find the solution to a corresponding linear matrix system - is incredibly important for the success of any algorithm.\n",
    "\n",
    "When we started talking about matrices we noted that as long as the matrix is non-singular, i.e. $\\det(A)\\ne 0$, then an inverse exists, and a linear system with that $A$ has a unique solution.\n",
    "\n",
    "But what happens when we consider a matrix that is *nearly* singular, i.e. $\\det(A)$ is very small?\n",
    "\n",
    "Well smallness is a relative term and so we need to ask the question of how large or small $\\det(A)$ is compared to something.\n",
    "\n",
    "That something is the *norm* of the matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector norms\n",
    "\n",
    "For a vector $\\boldsymbol{v}$ (assumed to be an $n\\times 1$ column vector) we have multiple possible norms to help us decide quantify the magnitude or size of that vector:\n",
    "\n",
    "\\begin{align}\n",
    "\\|\\boldsymbol{v}\\|_2 & = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2} = \\left(\\sum_{i=1}^n v_i^2 \\right)^{1/2}, &\\quad{\\textrm{termed the two-norm or Euclidean norm}}\\\\\n",
    "\\|\\boldsymbol{v}\\|_1  & = |v_1| + |v_2| + \\cdots + |v_n| = \\sum_{i=1}^n |v_i|, &\\quad{\\textrm{termed the one-norm or taxi-cab norm}}\\\\\n",
    "\\|\\boldsymbol{v}\\|_{\\infty}  &= \\max\\{|v_1|,|v_2|, \\ldots, |v_n|\\} = \\max_{i=1}^n |v_i|, &\\quad{\\textrm{termed the max-norm or infinity norm}}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix norms\n",
    "\n",
    "We can define similar measures for the size of matrices, e.g. for $A$ which for complete generality we will assume is of shape $m\\times n$:\n",
    "\n",
    "\\begin{align}\n",
    "\\|A\\|_F & = \\left(\\sum_{i=1}^m \\sum_{j=1}^n A_{ij}^2 \\right)^{1/2}, &\\quad{\\textrm{termed the matrix Euclidean or Frobenius norm}}\\\\\n",
    "\\|A\\|_{\\infty} & = \\max_{i=1}^m \\sum_{j=1}^n|A_{i,j}|, &\\quad{\\textrm{termed the maximum absolute row-sum norm}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Note that while these norms give different results (in both the vector and matrix cases), they are consistent or equivalent in that they are always within a constant factor of one another (a result that is true for finite-dimensional or discrete problems as we are considering here). \n",
    "\n",
    "This means we don't really need to worry too much about which norm we're using, as long as we are comparing vectors or matrices of the same size.\n",
    "\n",
    "[NB. if $n$ or $m$ changes for example and we want to fairly compare two vectors or matrices, then in the vector case the so-called *RMS* error would in many cases be a more suitable choice than the two-norm.]\n",
    "\n",
    "Let's evaluate some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.  2.  1.]\n",
      " [ 6.  5.  4.]\n",
      " [ 1.  4.  7.]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[10., 2., 1.],[6., 5., 4.],[1., 4., 7.]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.748015748023622\n"
     ]
    }
   ],
   "source": [
    "print(sl.norm(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.748015748023622\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# the \"Frobenius\" (or Euclidean) norm from above \n",
    "print(sl.norm(A,'fro'))\n",
    "# clearly the default for sl.norm as it returns the same answer as not specifying the norm\n",
    "print(sl.norm(A,'fro') == sl.norm(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0\n"
     ]
    }
   ],
   "source": [
    "# the maximum absolute row-sum\n",
    "print(sl.norm(A,np.inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.0\n"
     ]
    }
   ],
   "source": [
    "# the maximum absolute column-sum\n",
    "print(sl.norm(A,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.793091098640064\n"
     ]
    }
   ],
   "source": [
    "# the two-norm - note that this is NOT the same as the Frobenius norm\n",
    "# as might be expected by comparing the terminology for the vector and matrix norms\n",
    "# the matrix two-norm is also termed the spectral norm\n",
    "print(sl.norm(A,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.793091098640062\n"
     ]
    }
   ],
   "source": [
    "# which is defined as (!!!!)\n",
    "print(np.sqrt(np.real((np.max(sl.eigvals( A.T @ A))))))\n",
    "# i.e. involves the eigenvalues of the matrix A.T@A, \n",
    "# or the so-called \"singular values\" of A - see module on Geophysical Inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Exercise 4: matrix norms</span>\n",
    "\n",
    "Write some code to explicitly compute the two matrix norms defined mathematically above (i.e. the Frobenius and the maximum absolute row-sum norms) and compare against the values found above using in-built scipy functions.\n",
    "\n",
    "Also, based on the above code and comments, what is the mathematical definition of the 1-norm and the 2-norm?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix conditioning\n",
    "\n",
    "The (ill-)conditioning of a matrix is measured with the matrix condition number:\n",
    "\n",
    "$$\\textrm{cond}(A) = \\|A\\|\\|A^{-1}\\|$$\n",
    "\n",
    "If this is close to one then $A$ is termed *well-conditioned*; the value increases with the degree of *ill-conditioning*, reaching infinity for a singular matrix.\n",
    "\n",
    "Let's evaluate the condition number for the matrix above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.  2.  1.]\n",
      " [ 6.  5.  4.]\n",
      " [ 1.  4.  7.]]\n",
      "10.713371881346792\n",
      "10.713371881346786\n",
      "12.463616561943589\n",
      "12.463616561943587\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[10., 2., 1.],[6., 5., 4.],[1., 4., 7.]])\n",
    "\n",
    "print(A)\n",
    "print(np.linalg.cond(A))  # let's use the in-built condition number function\n",
    "print(sl.norm(A,2)*sl.norm(sl.inv(A),2))  # so the default condition number uses the matrix two-norm\n",
    "print(np.linalg.cond(A,'fro')) # this is how to use a different norm\n",
    "print(sl.norm(A,'fro')*sl.norm(sl.inv(A),'fro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition number is expensive to compute, and so in practice the relative size of the determinant of the matrix can be gauged based on the magnitude of the entries of the matrix.\n",
    "\n",
    "#### Example\n",
    "\n",
    "We know that a singular matrix does not result in a unique solution to its corresponding linear matrix system. But what are the consequences of near-singularity (ill-conditioning)?\n",
    "\n",
    "Consider the following example\n",
    "\n",
    "\n",
    "$$\n",
    "\\left(\n",
    "  \\begin{array}{cc}\n",
    "    2 & 1 \\\\\n",
    "    2 & 1 + \\epsilon  \\\\\n",
    "  \\end{array}\n",
    "\\right)\\left(\n",
    "  \\begin{array}{c}\n",
    "    x \\\\\n",
    "    y \\\\\n",
    "  \\end{array}\n",
    "\\right) = \\left(\n",
    "  \\begin{array}{c}\n",
    "    3 \\\\\n",
    "    0 \\\\\n",
    "  \\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Clearly when $\\epsilon=0$ the two columns/rows are not linear independent, and hence the determinant of this matrix is zero, the condition number is infinite, and the linear system does not have a solution (as the two equations would be telling us the contradictory information that $2x+y$ is equal to 3 and is also equal to 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Exercise 5: Ill-conditioned matrix</span>\n",
    "\n",
    "For the example above, consider a range of small values for $\\epsilon$ and calculate the matrix determinant and condition number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find for $\\epsilon=0.001$ that $\\det(A)=0.002$ (i.e. quite a lot smaller than the other coefficients in the matrix) and $\\textrm{cond}(A)\\approx 5000$.\n",
    "\n",
    "Using `sl.inv(A) @ b` you should also be able to compute the solution $\\boldsymbol{x}=(1501.5,-3000.)^T$.\n",
    "\n",
    "What happens when you make a very small change to the coefficients of the matrix (e.g. set $\\epsilon=0.002$)?\n",
    "\n",
    "You should find that this change of just $0.1\\%$ in one of the coefficients of the matrix (i.e. $1.001$ becoming $1.002$) results in a $100\\%$ change in both components of the solution!\n",
    "\n",
    "This is the consequence of the matrix being *ill-conditioned* - we should be careful about trusting the numerical solution to ill-conditioned problems.\n",
    "\n",
    "A way to see this is to recognise that computers do not perform arithmetic exactly - they necessarily have to [truncate numbers](http://www.mathwords.com/t/truncating_a_number.htm) at a certain number of significant figures, performing multiple operations with these truncated numbers can lead to an erosion of accuracy. Often this is not a problem, but these so-called [roundoff](http://mathworld.wolfram.com/RoundoffError.html) errors in algorithms generating $A$, or operating on $A$ as in Gaussian elimination etc, will lead to small inaccuracies in the coefficients of the matrix. Hence, in the case of ill-conditioned problems, will fall foul of the issue seen above where a very small error in an input to the algorithm led to a far larger error in an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roundoff errors\n",
    "\n",
    "Note that in this course we have largely ignored the limitations of the floating point arithmetic performed by computers, including round-off errors.  \n",
    "\n",
    "This is often the topic of the first lecture of courses, or first chapter of books, on Numerical Methods or Numerical Analysis - do take a look at some examples if you are interested.  \n",
    "\n",
    "Also take a look at *D. Goldberg 1991: What every computer scientist should know about floating-point arithmetic, ACM Computing Surveys 23, Pages 5-48*.\n",
    "\n",
    "For some examples of catastrophic failures due to round off errors see <https://www.ma.utexas.edu/users/arbogast/misc/disasters.html> and <http://ta.twi.tudelft.nl/users/vuik/wi211/disasters.html> and [the sinking of the Sleipner A offshore platform](http://www.ima.umn.edu/~arnold/disasters/sleipner.html).\n",
    "\n",
    "<br>\n",
    "\n",
    "As an example, consider the mathematical formula\n",
    "\n",
    "$$f(x)=(1-x)^{10}.$$\n",
    "\n",
    "We can of course relatively easily expand this out by hand\n",
    "\n",
    "$$f(x)=1- 10x + 45x^2 - 120x^3 + 210x^4 - 252x^5 + 210x^6 - 120x^7 + 45x^8 - 10x^9 + x^{10}.$$\n",
    "\n",
    "Mathematically these two expressions for $f(x)$ are identical; when evaluated by a computer different operations will be performed, which (we hope) should give the same answer. For numbers $x$ away from $1$ these two expressions do return (pretty much) the same answer.  \n",
    "\n",
    "However, for $x$ close to 1 the second expression adds and subtracts individual terms of increasing size which should largely cancel out, but they don't to sufficient accuracy due to round off errors; these errors accumulate with more and more operations, leading a loss of significance <https://en.wikipedia.org/wiki/Loss_of_significance>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00010485760000000006 0.00010485760000436464 4.1623815505431594e-11 \n",
      "\n",
      "1.0239999999999978e-07 1.0240001356576212e-07 1.3247813024364063e-07 \n",
      "\n",
      "9.765625000000086e-14 1.2378986724570495e-13 0.21111273343425307\n"
     ]
    }
   ],
   "source": [
    "def f1(x):\n",
    "    return (1. - x)**10\n",
    "\n",
    "def f2(x):\n",
    "    return (1. - 10.*x + 45.*x**2 - 120.*x**3 +\n",
    "           210.*x**4 - 252.*x**5 + 210.*x**6 -\n",
    "           120.*x**7 + 45.*x**8 - 10.*x**9 + x**10)\n",
    "\n",
    "# for a value of x away from 1\n",
    "x=0.6\n",
    "print(f1(x), f2(x), 1.-f1(x)/f2(x), '\\n') # print values computed in different ways and their relative difference\n",
    "# the error is far down the significant figures\n",
    "\n",
    "# things get a bit worse as x gets closer to 1\n",
    "x=0.8\n",
    "print(f1(x), f2(x), 1.-f1(x)/f2(x), '\\n') \n",
    "\n",
    "# and a significant error (21%) can be see for a number close to 1\n",
    "x=0.95\n",
    "print(f1(x), f2(x), 1.-f1(x)/f2(x)) \n",
    "# f2 is simply swamped with round off errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm stability\n",
    "\n",
    "The susceptibility for a numerical algorithm to dampen (inevitable) errors, rather than to magnify them as we have seen in examples above, is termed *stability*.  This is a concern for numerical linear algebra as considered here, as well as for the numerical solution of differential equations.  In that case you don't want small errors to grow and accumulate as you propagate the solution to an ODE or PDE forward in time say.\n",
    "\n",
    "If your algorithm is not inherently stable, or has other limitation, you need to understand and appreciate this, as it can cause catastrophic failures! \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "427.513px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
